# Documentation Crawler and PDF Converter

This project contains Python scripts to crawl and convert a technical documentation site into a single, consolidated PDF file. The process uses `chromedriver` for web automation and `wkhtmltopdf` for HTML-to-PDF conversion. 

## Prerequisites

Make sure you have **Homebrew** installed on your Mac. You can install it with:
```bash
/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"
Required Packages and Tools
Python 3: Make sure Python 3 is installed on your system.
Homebrew: Used to install chromedriver and wkhtmltopdf.

Project Setup

Clone the Repository
git clone https://github.com/yourusername/chronicle-doc-crawler.git
cd chronicle-doc-crawler

Create and Activate a Virtual Environment
python3 -m venv venv
source venv/bin/activate

Install Python Dependencies
pip install pyyaml pdfkit beautifulsoup4 requests tqdm

Install System Tools

chromedriver:
brew install --cask chromedriver
xattr -d com.apple.quarantine /opt/homebrew/bin/chromedriver

Verify the installation:
chromedriver --version

wkhtmltopdf:
brew install wkhtmltopdf

Confirm itâ€™s installed:
wkhtmltopdf --version

Usage

This project contains two scripts:

Crawler: Gathers links from Google Chronicle documentation and saves them to a YAML file.

Converter: Uses the YAML file to create a single PDF of the documentation.

Step 1: Run the Crawler
Run the crawler to generate a site structure in YAML format.
python3 crawler.py <start_url> --output site_structure.yaml

<start_url>: The starting URL for the documentation crawl (e.g., https://cloud.google.com/chronicle/docs/secops/secops-overview).
--output: Specifies the name of the output YAML file (e.g., site_structure.yaml).

The crawler will save all found links in a structured YAML file.

Step 2: Edit the YAML File
Open site_structure.yaml in a text editor and manually remove any sections or links that you do not want included in the final PDF.

Step 3: Run the Converter
After editing the YAML file, use the converter script to generate the PDF.

python3 convert.py site_structure.yaml --output combined_output.pdf

site_structure.yaml: The YAML file generated by the crawler, which you edited.

--output: Specifies the name of the output PDF file (e.g., combined_output.pdf).

The script will:
Fetch each page listed in site_structure.yaml.
Clean up headers and footers.
Convert each page to an individual PDF.
Combine all individual PDFs into one PDF.

Result
The output file (combined_output.pdf) will contain all selected Google Chronicle documentation pages in a single, formatted PDF.
